\chapter{Sequences and series of functions}

\section{Pointwise and uniform convergence}

Previously, we have defined and dealt with the convergence of a sequence of numbers. So, we must begin with an understanding of what it means to a sequence of functions to converge.

\begin{definition}[Pointwise convergence]
    Define $f:S \to \R$ and $f_n: S \to \R$ with $n \in \N$. The sequence of functions $(f_n)$ is said to \emph{converge pointwise} to $f$ if:
    \begin{equation*}
        \lim \limits_{n \to \infty} f_n(x) = f(x), \forall x \in S
    \end{equation*}
\end{definition}

\begin{eg}
    \begin{equation*}
        \lim \limits_{n \to \infty} f_n(x) = \begin{cases}
            0 \text{ if } x \in [0,1) \\
            1 \text{ if } x=1
        \end{cases}
    \end{equation*}
    Thus $\{f_n(x)\}$ converges pointwise to the function above, hence a sequence of continuous function may not converge to a continuous function.       
\end{eg}

\begin{eg}
    Consider $f_n(x) = \sum_{m=0}^n x^m$ for $x \in (-1,1)$. Then,
            \begin{equation*}
                \lim \limits_{n \to \infty} f_n(x) = \lim \limits_{n \to \infty} \sum \limits_{m=0}^n x_m = \frac{1}{1-x}
            \end{equation*}
\end{eg}

\begin{definition}[Uniform convergence]
    For $n \in \N$, define $f_n: S \to \R$ and $f: S \to \R$. Then, $f_n$ \emph{converges uniformly} to $f$ if $\forall \varepsilon > 0, \exists N \in \N$ such that $|f_n(x) - f(x)| < \varepsilon, \forall n \geq N, \forall x \in S$.
\end{definition}

\begin{theorem}
    Consider $f_n: S \to \R$ and $f: S \to \R$. Then, if $f_n$ converges to $f$ uniformly it also converges to $f$ pointwise.
\end{theorem}

\begin{proof}
    Consider $c \in S$ and let $\varepsilon > 0$. Then, if $f_n \to f$ uniformly, $\exists N \in \N$ such that $|f_n(x) - f(x)| < \varepsilon, \forall n \geq N, \forall x \in S$. So, $\lim_{n \to \infty} f_n(c) = f(c), \forall c \in S$. Therefore, $f_n$ converges to $f$ pointwise.
\end{proof}

\begin{theorem}[Cauchy criterion for uniform convergence]
    Consider $S \subseteq \R$, non-empty. Let $f_n: S \to \R$ be a sequence of functions. Then, $f_n$ converges to $f$ uniformly on $S$ if $\forall \varepsilon > 0, \exists N \in \N$ such that:
    \begin{equation*}
        |f_n(x) - f_m(x)| < \varepsilon, \forall n, m \geq N, \forall x \in S
    \end{equation*}
\end{theorem}

\begin{proof}
    Suppose $f_n$ converges uniformly to $f$ on $S$, as $n \to \infty$. Then, $\forall \varepsilon > 0, \exists N \in \N$ such that:
    \begin{equation*}
        |f_n(x) - f(x)| < \frac{\varepsilon}{2}, \forall n \geq N, \forall x \in S
    \end{equation*}
    On the other hand,
    \begin{align*}
        |f_n(x) - f_m(x)| &= |f_n(x) - f(x) + f(x) - f_m(x)| \\
        &\leq |f_n(x) - f(x)| + |f(x) - f_m(x)| \\
        &\leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
        &= \varepsilon
    \end{align*}
    If $|f_n(x) - f_m(x)| < \varepsilon, \forall \varepsilon > 0, \forall x \in S$, then $(f_n(x))_n$ is Cauchy. So,
    \begin{equation*}
        f(x) := \lim \limits_{n \to \infty} f_n(x), \forall x \in S
    \end{equation*}
    from the Cauchy theorem of sequences. Additionally,
    \begin{equation*}
        \lim \limits_{m \to \infty} |f_n(x) - f_m(x)| = |f_n(x) - f(x)| < \frac{\varepsilon}{2} < \varepsilon, \forall n \geq N, \forall x \in S
    \end{equation*}
    So, from the definition, $f_n \to f$ uniformly.
\end{proof}

\begin{theorem}[Weierstrass M-test]
    Define $f_i: S \to \R$ and suppose $\exists M_i > 0$ such that:
    \begin{itemize}
        \item $|f_i(x)| < M_i, \forall x \in S$,
        \item $\sum_{i=1}^\infty M_i$
    \end{itemize}
    Then,
    \begin{enumerate}
        \item $\sum_{i=1}^\infty f_i(x)$ converge absolutely for all $x \in S$,
        \item Define $f(x) = \sum_{i = 1}^\infty f_i(x), \forall x \in S$, then $\sum_{i=1}^n f_i$ converges uniformly to $f$ on $S$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Proving each statement:
    \begin{enumerate}
        \item The first item implies the sequence $\{|f_i(x)|\}_i$ is bounded for all $x \in S$ by $M_i$. So,
            \begin{equation*}
                \sum \limits_{i=1}^n |f_i(x)| \leq \sum \limits_{i=1}^n M_i
            \end{equation*}
        By the comparison test, the sequence $\{\sum_{i=1}^n |f_i(x)|\}_n$ converges absolutely, since $\sum_{i=1}^\infty M_i$ converges. Hence, $\sum_{i=1}^\infty f_i(x)$ converges absolutely for all $ x \in S$.
        \item Let $\varepsilon > 0$. Since $\sum M_i$ converges, then $\exists N_0 \in \N$ such that:
            \begin{equation*}
                \sum \limits_{i = n+1}^\infty M_i = \left |
                    \sum \limits_{i = 1}^\infty M_i
                    - \sum \limits_{i = 1}^n M_i
                \right | < \varepsilon, \forall n \geq N_0
            \end{equation*}
            Take $N = N_0$. Then,
            \begin{align*}
                \left |
                    f(x) - \sum \limits_{i = 1}n f_i(x)
                \right |
                &= \left |
                    \sum \limits_{i = n+1}^\infty f_i(x)
                \right | \\
                &\leq \sum \limits_{i = n+1}^\infty |f_i(x)| \\
                &\leq \sum \limits_{i = n+1}^\infty M_j \\
                &< \varepsilon, \forall n \geq N, \forall x \in S
            \end{align*}
            So, $\sum_{i=1}^n f_i(x) \to f(x), \forall x \in S$.
    \end{enumerate}
\end{proof}

\section{Properties of the convergent sequence of functions}

One of the central problems related to sequences or series of functions focus on whether the limit (or derivative, or integral) can be calculated by the term-by-term operation over the series expansion. As the previous chapters have hopefully shown, derivatives and integrals are limit operations, as is the process of taking the sum of an infinite series. The multiple limit operation over an object can be problematic.

\begin{remark}
    In general, interchanging limits leads to different results.    
\end{remark}

\begin{eg}
    For instance, consider:
    \begin{eqnarray*}
        \lim \limits_{n \to \infty} \lim \limits_{k \to \infty} \frac{n/k}{n/k + 1} &=& \lim \limits_{n \to \infty} \frac{0}{0+1} = 0 \\
        \lim \limits_{k \to \infty} \lim \limits_{n \to \infty} \frac{n/k}{n/k + 1} &=& \lim \limits_{k \to \infty} 1 = 1
    \end{eqnarray*}
\end{eg}

So, there are a few questions that could be made about interchanging limits applied to sequences of functions:
\begin{enumerate}
    \item If $f_n: S \to \R$ is a sequence of continuous functions such that $f_n$ converges to $f$ pointwise or uniformly, then is $f$ continuous?
    \item If $f_n: [a,b] \to \R$ is a sequence of continuous differentiable functions such that $f_n$ converges to $f$ and $f'_n \to g$, then is $f$ differentiable and does $g(x) = f'(x)$?
    \item If $f_n: [a,b] \to \R$ with $f_n$ and $f$ continuous such that $f_n$ converges to $f$, then does
        \begin{equation*}
            \int_a^b f_n(x) \dint x = \int_a^b f(x) \dint x \text{ ?}
        \end{equation*}
\end{enumerate}

The answer to the previous questions is yes, provided the convergence is uniform. Else, the answer is no if the convergence is pointwise. This last result can be proved by counterexamples:

\begin{eg}
    Finding counterexamples for each question:
    \begin{enumerate}
        \item Consider $f_n(x) = x^n$ on $[0,1]$. Then $f_n(x)$ is continuous for all $n$. As discussed earlier:
            \begin{equation*}
                f_n(x) \to f(x) = \begin{cases}
                    0 \text{ if } x \in [0,1) \\
                    1 \text{ if } x = 1
                \end{cases}
            \end{equation*}
        Notice $f(x)$ is not continuous, so the answer to the first of the three previous questions is negative.
        \item Consider $f_n(x) = \frac{x^{n+1}}{n+1}$ on $[0,1]$. Then $f_n(x)$ converges to $0$ pointwise on $[0,1]$. On the other hand,
            \begin{equation*}
                f'_n(x) \to g(x) = \begin{cases}
                    0 \text{ if } x \in [0,1) \\
                    1 \text{ if } x = 1
                \end{cases}
            \end{equation*}
            Thus, $g(x) \neq (0)' = 0$ at $x=1$. So, the second question has a negative answer for pointwise convergence.
        \item Consider
            \begin{equation*}
                f_n(x) = \begin{cases}
                    4n^2x \qquad \:\:\: \text{ if } x \in \left [ 0, \frac{1}{2n} \right ) \\
                    4n - 4n^2x \ \text{ if } x \in \left [\frac{1}{2n}, \frac{1}{n} \right ) \\
                    0 \qquad \qquad \: \text{ if } x \in \left [ \frac{1}{n}, 1 \right ]
                \end{cases}
            \end{equation*}
            Then $f_n(x)$ converges to $0$ pointwise on $[0,1]$. However,
            \begin{equation*}
                \int_0^1 f_n(x) \dint x = \frac{1}{2n}2n = 0 \not \to 0 = \int_0^1 0 \dint x
            \end{equation*}
            So, the answer to the third question is no for the case of pointwise convergence.
    \end{enumerate}
\end{eg}
\vspace{1em}
Next, we must show the answer to the previous questions is positive in the case of uniform convergence.

\begin{theorem}
    If $f_n:S \to \R$ is continuous for all $n \in \N$. And $f: S \to \R$, such that $f_n$ converges to $f$ uniformly, then $f$ is continuous.
\end{theorem}

\begin{proof}
    Let $c \in S$ and $\varepsilon > 0$. Since $f_n$ converges to $f$ uniformly, $\exists N \in \N$ such that $|f_n(y) - f(y)| < \varepsilon/3, \forall n \geq N, \forall y \in S$. Since $f_N: S \to \R$ is continuous then $\exists \varepsilon_0 > 0$ such that $|f_N(x) - f_N(c)| < \varepsilon/3, \forall x \in (c - \delta_0, c + \delta_0)$. Choose $\delta = \delta_0$, then:
    \begin{align*}
        |f(x) - f(c)| &\leq |f(x) - f_N(x)| + |f_N(c) - f(c)| + |f_N(x) - f_N(c)| \\
        &< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
        &= \varepsilon
    \end{align*}
\end{proof}

\begin{theorem}
    If $f_n: [a,b] \to \R$ is a sequence of continuous functions such that $f_n$ converges to $f: [a,b] \to \R$ uniformly then
    \begin{equation*}
        \int_a^b f_n(x) \dint x \to \int_a^b f(x) \dint x
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $\varepsilon > 0$. Since $f_n \to f$ uniformly, then $\exists N \in \N$ such that:
    \begin{equation*}
        |f_n(x) - f(x)| < \frac{\varepsilon}{b-a}, \forall n \geq N, \forall x \in [a,b]
    \end{equation*}
    Then,
    \begin{equation*}
        \left |
            \int_a^b f_n(x) \dint x- \int_a^b f(x) \dint x 
        \right | \leq 
        \int_a^b |f_n(x) - f(x)| \dint x 
        <
        \int_a^b \frac{\varepsilon}{b-a} = \varepsilon
    \end{equation*}
\end{proof}

\begin{remark}
    Notationally, this is equivalent to:
    \begin{equation*}
        \lim \limits_{n \to \infty} \int_a^b f_n(x) \dint x = 
        \int_a^b \lim \limits_{n \to \infty} f_n(x) \dint x = 
        \int_a^b f(x) \dint x
    \end{equation*}
\end{remark}

\begin{theorem}
    If $f_n: [a,b] \to \R$ is a sequence of continuous differentiable functions, $f: [a,b] \to \R$, $g:[a,b] \to \R$, and:
    \begin{enumerate}
        \item $f_n$ converges to $f$ pointwise,
        \item $f'_n$ converges to $g$ uniformly,
    \end{enumerate}
    then $f$ is continuously differentiable on $[a,b]$ and $g(x) = f'(x)$.
\end{theorem}

\begin{proof}
    By the Fundamental theorem of Calculus,
    \begin{equation*}
        f_n(x) - f(x) = \int_a^x f'_n(t)\dint t, \forall n \in \N, \forall x \in [a,b]
    \end{equation*}
    Thus, by the previous two theorems,
    \begin{align*}
        f(x) - f(a) &= \lim \limits_{n \to \infty} (f_n(x) - f_n(a)) \\
        &= \lim \limits_{n \to \infty} \int_a^x f'_n(t) \dint t \\
        &= \int_a^x g(t) \dint t
    \end{align*}
    Therefore, $f(x) = f(a) + \int_a^x g(t) \dint t$. Thus, by the Fundamental theorem of calculus, $f$ is differentiable and $f'(x) = (\int_a^x g(t) \dint t)' = g(x)$.
\end{proof}

\section{Power series}

Power series are a natural generalization of polynomials.

\begin{definition}[Power series]
    A power series about $x_0$ is series of the form:
    \begin{equation*}
        \sum \limits_{n = 0}^\infty a_n(x - x_0)^n
    \end{equation*}
\end{definition}

\begin{remark}
    Under this setting, it is common to take $(x-x_0)^0 = 1$. Even though, $0^0$ is indeterminate.
\end{remark}

It is clear from the definition of power series that the series converges for $x=x_0$ to $a_0$. One question follows naturally from this observation: ``for what values of $x$ the power series converges?''

\begin{theorem}
    Suppose $(|a_n|^{1/n})$ converges, \emph{i.e.}:
    \begin{equation*}
        R = \lim \limits_{n \to \infty} |a_n|^{1/n}
    \end{equation*}
    and define $p$, the \emph{radius of convergence}, as:
    \begin{equation*}
        p = \begin{cases}
            \frac{1}{R} \text{ if } R > 0 \\
            \infty \text{ if } R = 0
        \end{cases}
    \end{equation*}
    Then:
    \begin{enumerate}
        \item If $|x-x_0| < p$, the series $\sum a_n(x - x_0)^n$ converges;
        \item If $|x-x_0| > p$, the series diverges.
    \end{enumerate}
\end{theorem}

\begin{proof}
    First, notice that:
    \begin{equation*}
        \lim \limits_{n \to \infty} |a_n(x-x_0)^n|^{1/n} = R |x-x_0|
    \end{equation*}
    So, the theorem is valid from the Root test.
\end{proof}

\begin{remark}
    There exists power series which converge only at one point.
\end{remark}

Suppose $\sum a_n(x-x_0)^n$ is a power series with radius of convergence $p$. Then, define $f: (x_0 - p, x_0 + p) \to \R$ such that:
\begin{equation*}
    f(x) := \sum \limits_{n=0}^\infty a_n (x - x_0)^n
\end{equation*}

So, $f(x)$ is a limit of a sequence of functions $f_n(x)$, \emph{i.e.}:

\begin{equation*}
    f(x) = \lim \limits_{m \to \infty} f_m(x)
\end{equation*}

Where,

\begin{equation*}
    f_m(x) = \sum \limits_{n=0}^m a_n(x-x_0)^n
\end{equation*}

\begin{theorem}
    Let $\sum_{i=0}^\infty a_i(x - x_0)^i$ be a power series with radius of convergence $p \in (0, \infty]$. Then,  $\forall r \in (0, p), \sum_{i=0}^\infty a_i(x - x_0)^i$ converges uniformly on $[x_0 - r, x_0 + r]$.
\end{theorem}

\begin{proof}
    Let $r \in [0, p)$. Then, $\forall i \in \N \cup \{0\}, \forall x \in [x_0 - r, x_0 + r], |a_i(x-x_0)^i| \leq |a_i|r^i = M_i$. Now,
    \begin{equation*}
        \lim \limits_{i \to \infty} M_i^{1/i} = \lim \limits_{i \to \infty} |a_i|^{1/i} r = \begin{cases}
            \frac{r}{p} \text{ if } p < \infty \\
            0 \text{ if } p = \infty
        \end{cases}
    \end{equation*}
    since $1/p = \lim_{i \to \infty} |a_i|^{1/i}$. Since $r < p$,
    \begin{equation*}
        \lim \limits_{i \to \infty} M_i^{1/i} < 1 \Longrightarrow \sum \limits_{i=1}^\infty M_i \text{ converges.}
    \end{equation*}
    By the Weierstrass M-test, $\sum_{i=0}^\infty a_i(x-x_0)^i$ converges uniformly on $[x_0 - r, x_0 + r]$.
\end{proof}

\begin{theorem}
    Consider $\sum_{i=0}^\infty a_i(x - x_0)^i$, a power series with radius of convergence $p \in (0, \infty]$. Then,
    \begin{enumerate}
        \item $\forall c \in (x_0 - p, x_0 + p), \sum_{i=0}^\infty a_i(x - x_0)^i$ is differentiable at $c$ and:
            \begin{equation*}
                \frac{\dint}{\dint x} \sum \limits_{i=0}^\infty a_i(x - x_0)^i = \sum \limits_{i=0}^\infty i a_i (x - x_0)^{i-1}
            \end{equation*}
        \item $\forall a, b$ such that $x_o - p < a <  b < x_0 + p$, then:
            \begin{equation*}
                \int_a^b \sum \limits_{i = 0}^\infty a_i(x- x_0)^i = \sum \limits_{i=0}^\infty \left(
                    \frac{(b-x_0)^{i+1}}{i+1} - \frac{(a - x_0)^{i+1}}{i + 1}
                \right)
            \end{equation*}
    \end{enumerate}
\end{theorem}

\begin{remark}
    Since 
    \begin{equation*}
        \lim \limits_{i \to \infty} ((i+1)|a_{i+1}|)^{1/i} = \lim \limits_{i\to \infty} \left(
            (i+1)|a_{i+1}|^{1/(i+1)}
        \right)^{(i+1)/i} = \lim \limits_{k \to \infty} |a_k|^{1/k} = p
    \end{equation*}
    the first result of the previous theorem implies $\sum a_i(x - x_0)^i$ is infinitely differentiable and:
    \begin{equation*}
        k! a_k = \left(
            \frac{\dint^k}{\dint x^k} \sum a_i (x - x_0)^i
        \right) \Bigg |_{x = x_0}
    \end{equation*}
\end{remark}

\begin{theorem}[Weierstrass approximation theorem]
    If $f \in C([a,b])$, there exists a sequence of polynomials $\{ P_n\}$ such that $P_n \to f$ uniformly on $[a,b]$.
\end{theorem}

\begin{theorem}
    Define $c_n := (\int_{-1}^1 (1 - x^2)^n \dint x)^{-1} > 0$, and let $Q_n(x) = c_n(1 - x^2)^n$. Then:
    \begin{enumerate}
        \item $\int_{-1}^1 Q_n(x) \dint x = 1, \forall n \in \N$,
        \item $Q_n(x) \geq 0, \forall n \in \N, \forall x \in [-1, 1]$, and
        \item $\forall \delta \in (0,1), Q_n \to 0$ uniformly on $\delta \leq |x| \leq 1$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Proving each statement:
    \begin{enumerate}
        \item $\int_{-1}^1 Q_n(x) \dint x = c_n \int_{-1}^1 (1 - x^2)^n \dint x = 1$ by definition of $c_n$,
        \item Since $c_n$ is strictly positive, notice $(1 - x^2) \geq 0, \forall x \in [-1,1]$. Any non-negative number to the $n$-th power will result in a non-negative number (since $n \in \N $). So $Q_n(x) \geq 0, \forall n \in \N, \forall x \in [-1, 1]$,
        \item First, notice:
            \begin{equation*}
                (1 - x^2)^n \geq 1 - nx^2, \forall n \in \N, \forall x \in [-1, 1]
            \end{equation*}
            This can be proven by induction, beyond the scope of this particular proof. And $g(x) = (1 - x^2)^n - (1 - n x^2)$ satisfies $g(0) = 0$. Additionally, $g'(x) = n 2 x (1 - (1 - x^2)^{n-1}) \geq 0$ in $[0,1]$. Thus, $g(x) \geq 0$ by the Mean value theorem. So,
            \begin{align*}
                \frac{1}{c+n} &= \int_{-1}^1 (1 - x^2)^n \dint x \\
                &= 2 \int_0^1 (1 - x^2)^n \dint x \\
                &> 2 \int_0^{1/\sqrt{n}}(1 - x^2)^n \dint x \\
                &\geq 2 \int_0^{1/\sqrt{n}}(1 - n x^2) \dint x \\
                &= 2 \left(
                    \frac{1}{\sqrt{n}} - \frac{n}{3}n^{-3/2}
                \right) \\
                &= \frac{4}{3}\sqrt{n}
                &>\sqrt{n}
            \end{align*}
            Therefore $c_n < \sqrt{n}$. Let $\delta > 0$. First, notice $\lim_{n \to \infty} \sqrt{n}(1 - \delta^2)^n = 0$. By the root test,
            \begin{align*}
                \lim \limits_{n \to \infty} (\sqrt{n} (1 - \delta^2)^n)^{1/n} &= \lim \limits_{n \to \infty} (n^{1/n})^{1/2}(1 - \delta^2) \\
                &= 1 - \delta^2 \\
                &< 1
            \end{align*}
            So, $\lim_{n \to \infty} \sqrt{n}(1 - \delta^2)^n = 0$. Let $\varepsilon > 0$ and choose $N \in \N$ such that $\sqrt{n}(1 - \delta^2)^n < \varepsilon, \forall n \geq N$. Then, $\forall n \geq N, \forall \delta \leq |x| \leq 1, |c_n(1-x^2)^n| < sqrt{n}(1 - x^2)^n \leq \sqrt{n}(1 - \delta^2)^n < \varepsilon$.
    \end{enumerate}
\end{proof}
\vspace{1em}
Finally, we return to proving Weierstrass approximation theorem.
\begin{proof}
    Suppose $f \in C([0, 1])$, with $f(0) = f(1) = 0$. $f$ can be extended to $C(\R)$ by setting $f(x) = 0, \forall x \not \in [0,1]$. Define:
    \begin{align*}
        P_n(x) &= \int_0^1 f(t) Q_n(t - x) \dint t \\
        &= \int_0^1 f(t) c_n (1 - (t - x)^2)^n \dint t
    \end{align*}
    Note that $P_n(x)$ is in fact a polynomial. Additionally, note that for $x \in [0,1]$,
    \begin{align*}
        P_n(x) &= \int_0^1 f(t) Q_n(t - x) \dint t \\
        &= \int_{-x}^{1 - x} f(x + t)Q_n(t) \dint t \\
        &= \int_{-1}^1 f(x + t)Q_n(t)\dint t
    \end{align*}
    The second equality follows from change of variables, the third one comes from $f(x + t) = 0, \forall t \not \in [-x, 1 - x]$.
    Now, it is necessary to show $P_n \to f$ uniformly on $[0,1]$. Let $\varepsilon > 0$, since $f$ is uniformly continuous on $[0,1], \exists \delta > 0$ such that $\forall |x-y| \leq \delta, |f(x) - f(y)| < \varepsilon/2$. Let $C = \sup \{f(x):x \in [0,1]\}$, which exists by the Min/Max theorem. Choose $N \in \N$ such that $\sqrt{n}(1 - \delta^2)^n < \varepsilon/(8C)$. Thus, $\forall n \geq N, \forall x \in [0,1]$,
    \begin{align*}
        |P_n(x) - f(x)| &= \left |
            \int_{-1}^1 (f(x-t) - f(t))Q_n(t) \dint t
        \right | \\
        & \leq \int_{-1}^1 |f(x - t) - f(x)| Q_n(t) \dint t \\
        & \leq \int_{|t| \leq \delta} |f(x - t) - f(x)| Q_n(t) \dint t + \int_{\delta \leq |t| \leq 1} |f(x - t) - f(x)| Q_n(t) \dint t \\
        & \leq \frac{\varepsilon}{2}\int_{|t| \leq \delta} Q_n(t) \dint t + \sqrt{n}(1 - \delta^2)^n \int_{\delta \leq |t| \leq 1} 2C \dint t \\
        &< \frac{\varepsilon}{2} + 4C \sqrt{n}(1 - \delta^2)^n \\
        &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
        &= \varepsilon
    \end{align*}
\end{proof}