\chapter{Sequences and series of functions}

\section{Pointwise and uniform convergence}

Previously, we have defined and dealt with the convergence of a sequence of numbers. So, we must begin with an understanding of what it means to a sequence of functions to converge.

\subsection{Sequence of functions convergence}

\begin{definition}[Pointwise convergence]
    Define $f:S \to \R$ and $f_n: S \to \R$ with $n \in \N$. The sequence of functions $(f_n)$ is said to \emph{converge pointwise} to $f$ if:
    \begin{equation*}
        \lim \limits_{n \to \infty} f_n(x) = f(x), \forall x \in S
    \end{equation*}
\end{definition}

\begin{eg}
    \begin{equation*}
        \lim \limits_{n \to \infty} f_n(x) = \begin{cases}
            0 \text{ if } x \in [0,1) \\
            1 \text{ if } x=1
        \end{cases}
    \end{equation*}
    Thus $\{f_n(x)\}$ converges pointwise to the function above, hence a sequence of continuous function may not converge to a continuous function.       
\end{eg}

\begin{eg}
    Consider $f_n(x) = \sum_{m=0}^n x^m$ for $x \in (-1,1)$. Then,
            \begin{equation*}
                \lim \limits_{n \to \infty} f_n(x) = \lim \limits_{n \to \infty} \sum \limits_{m=0}^n x_m = \frac{1}{1-x}
            \end{equation*}
\end{eg}

\begin{definition}[Uniform convergence]
    For $n \in \N$, define $f_n: S \to \R$ and $f: S \to \R$. Then, $f_n$ \emph{converges uniformly} to $f$ if $\forall \varepsilon > 0, \exists N \in \N$ such that $|f_n(x) - f(x)| < \varepsilon, \forall n \geq N, \forall x \in S$.
\end{definition}

\begin{theorem}
    Consider $f_n: S \to \R$ and $f: S \to \R$. Then, if $f_n$ converges to $f$ uniformly it also converges to $f$ pointwise.
\end{theorem}

\begin{proof}
    Consider $c \in S$ and let $\varepsilon > 0$. Then, if $f_n \to f$ uniformly, $\exists N \in \N$ such that $|f_n(x) - f(x)| < \varepsilon, \forall n \geq N, \forall x \in S$. So, $\lim_{n \to \infty} f_n(c) = f(c), \forall c \in S$. Therefore, $f_n$ converges to $f$ pointwise.
\end{proof}

\subsection{Convergence criteria and test}

\begin{theorem}[Cauchy criterion for uniform convergence]
    Consider $S \subseteq \R$, non-empty. Let $f_n: S \to \R$ be a sequence of functions. Then, $f_n$ converges to $f$ uniformly on $S$ if $\forall \varepsilon > 0, \exists N \in \N$ such that:
    \begin{equation*}
        |f_n(x) - f_m(x)| < \varepsilon, \forall n, m \geq N, \forall x \in S
    \end{equation*}
\end{theorem}

\begin{proof}
    Suppose $f_n$ converges uniformly to $f$ on $S$, as $n \to \infty$. Then, $\forall \varepsilon > 0, \exists N \in \N$ such that:
    \begin{equation*}
        |f_n(x) - f(x)| < \frac{\varepsilon}{2}, \forall n \geq N, \forall x \in S
    \end{equation*}
    On the other hand,
    \begin{align*}
        |f_n(x) - f_m(x)| &= |f_n(x) - f(x) + f(x) - f_m(x)| \\
        &\leq |f_n(x) - f(x)| + |f(x) - f_m(x)| \\
        &\leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
        &= \varepsilon
    \end{align*}
    If $|f_n(x) - f_m(x)| < \varepsilon, \forall \varepsilon > 0, \forall x \in S$, then $(f_n(x))_n$ is Cauchy. So,
    \begin{equation*}
        f(x) := \lim \limits_{n \to \infty} f_n(x), \forall x \in S
    \end{equation*}
    from the Cauchy theorem of sequences. Additionally,
    \begin{equation*}
        \lim \limits_{m \to \infty} |f_n(x) - f_m(x)| = |f_n(x) - f(x)| < \frac{\varepsilon}{2} < \varepsilon, \forall n \geq N, \forall x \in S
    \end{equation*}
    So, from the definition, $f_n \to f$ uniformly.
\end{proof}

\begin{theorem}[Weierstrass M-test]
    Define $f_n: S \to \R$ and suppose $\exists M_n > 0, \forall n \in \N$ such that:
    \begin{itemize}
        \item $|f_n(x)| < M_n, \forall x \in S$,
        \item $\sum_{n=1}^\infty M_n$ converges
    \end{itemize}
    Then,
    \begin{enumerate}
        \item $\sum_{n=1}^\infty f_n(x)$ converge absolutely for all $x \in S$,
        \item Define $f(x) = \sum_{n = 1}^\infty f_n(x), \forall x \in S$, then $\sum_{n=1}^\infty f_n$ converges uniformly to $f$ on $S$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Proving each statement:
    \begin{enumerate}
        \item The first item implies the sequence $\{|f_i(x)|\}_i$ is bounded for all $x \in S$ by $M_i$. So,
            \begin{equation*}
                \sum \limits_{i=1}^n |f_i(x)| \leq \sum \limits_{i=1}^n M_i
            \end{equation*}
        By the comparison test, the sequence $\{\sum_{i=1}^n |f_i(x)|\}_n$ converges absolutely, since $\sum_{i=1}^\infty M_i$ converges. Hence, $\sum_{i=1}^\infty f_i(x)$ converges absolutely for all $ x \in S$.
        \item Let $\varepsilon > 0$. Since $\sum M_i$ converges, then $\exists N_0 \in \N$ such that:
            \begin{equation*}
                \sum \limits_{i = n+1}^\infty M_i = \left |
                    \sum \limits_{i = 1}^\infty M_i
                    - \sum \limits_{i = 1}^n M_i
                \right | < \varepsilon, \forall n \geq N_0
            \end{equation*}
            Take $N = N_0$. Then,
            \begin{align*}
                \left |
                    f(x) - \sum \limits_{i = 1}n f_i(x)
                \right |
                &= \left |
                    \sum \limits_{i = n+1}^\infty f_i(x)
                \right | \\
                &\leq \sum \limits_{i = n+1}^\infty |f_i(x)| \\
                &\leq \sum \limits_{i = n+1}^\infty M_j \\
                &< \varepsilon, \forall n \geq N, \forall x \in S
            \end{align*}
            So, $\sum_{i=1}^n f_i(x) \to f(x), \forall x \in S$.
    \end{enumerate}
\end{proof}

\section{Properties of convergent sequence of functions}

One of the central problems related to sequences or series of functions focus on whether the limit (or derivative, or integral) can be calculated by the term-by-term operation over the series expansion. As the previous chapters have hopefully shown, derivatives and integrals are limit operations, as is the process of taking the sum of an infinite series. The multiple limit operation over an object can be problematic.

\begin{remark}
    In general, interchanging limits leads to different results.    
\end{remark}

\begin{eg}
    For instance, consider:
    \begin{eqnarray*}
        \lim \limits_{n \to \infty} \lim \limits_{k \to \infty} \frac{n/k}{n/k + 1} &=& \lim \limits_{n \to \infty} \frac{0}{0+1} = 0 \\
        \lim \limits_{k \to \infty} \lim \limits_{n \to \infty} \frac{n/k}{n/k + 1} &=& \lim \limits_{k \to \infty} 1 = 1
    \end{eqnarray*}
\end{eg}

So, there are a few questions that could be made about interchanging limits applied to sequences of functions:
\begin{enumerate}
    \item If $f_n: S \to \R$ is a sequence of continuous functions such that $f_n$ converges to $f$ pointwise or uniformly, then is $f$ continuous?
    \item If $f_n: [a,b] \to \R$ is a sequence of continuous differentiable functions such that $f_n$ converges to $f$ and $f'_n \to g$, then is $f$ differentiable and does $g(x) = f'(x)$?
    \item If $f_n: [a,b] \to \R$ with $f_n$ and $f$ continuous such that $f_n$ converges to $f$, then does
        \begin{equation*}
            \int_a^b f_n(x) \dint x = \int_a^b f(x) \dint x \text{ ?}
        \end{equation*}
\end{enumerate}

The answer to the previous questions is yes, provided the convergence is uniform. Else, the answer is no if the convergence is pointwise. This last result can be proved by counterexamples:

\begin{eg}
    Finding counterexamples for each question:
    \begin{enumerate}
        \item Consider $f_n(x) = x^n$ on $[0,1]$. Then $f_n(x)$ is continuous for all $n$. As discussed earlier:
            \begin{equation*}
                f_n(x) \to f(x) = \begin{cases}
                    0 \text{ if } x \in [0,1) \\
                    1 \text{ if } x = 1
                \end{cases}
            \end{equation*}
        Notice $f(x)$ is not continuous, so the answer to the first of the three previous questions is negative.
        \item Consider $f_n(x) = \frac{x^{n+1}}{n+1}$ on $[0,1]$. Then $f_n(x)$ converges to $0$ pointwise on $[0,1]$. On the other hand,
            \begin{equation*}
                f'_n(x) \to g(x) = \begin{cases}
                    0 \text{ if } x \in [0,1) \\
                    1 \text{ if } x = 1
                \end{cases}
            \end{equation*}
            Thus, $g(x) \neq (0)' = 0$ at $x=1$. So, the second question has a negative answer for pointwise convergence.
        \item Consider
            \begin{equation*}
                f_n(x) = \begin{cases}
                    4n^2x \qquad \:\:\: \text{ if } x \in \left [ 0, \frac{1}{2n} \right ) \\
                    4n - 4n^2x \ \text{ if } x \in \left [\frac{1}{2n}, \frac{1}{n} \right ) \\
                    0 \qquad \qquad \: \text{ if } x \in \left [ \frac{1}{n}, 1 \right ]
                \end{cases}
            \end{equation*}
            Then $f_n(x)$ converges to $0$ pointwise on $[0,1]$. However,
            \begin{equation*}
                \int_0^1 f_n(x) \dint x = \frac{1}{2n}2n = 0 \not \to 0 = \int_0^1 0 \dint x
            \end{equation*}
            So, the answer to the third question is no for the case of pointwise convergence.
    \end{enumerate}
\end{eg}
\vspace{1em}
Next, we must show the answer to the previous questions is positive in the case of uniform convergence.

\begin{theorem}
    If $f_n:S \to \R$ is continuous for all $n \in \N$. And $f: S \to \R$, such that $f_n$ converges to $f$ uniformly, then $f$ is continuous.
\end{theorem}

\begin{proof}
    Let $c \in S$ and $\varepsilon > 0$. Since $f_n$ converges to $f$ uniformly, $\exists N \in \N$ such that $|f_n(y) - f(y)| < \varepsilon/3, \forall n \geq N, \forall y \in S$. Since $f_N: S \to \R$ is continuous then $\exists \varepsilon_0 > 0$ such that $|f_N(x) - f_N(c)| < \varepsilon/3, \forall x \in (c - \delta_0, c + \delta_0)$. Choose $\delta = \delta_0$, then:
    \begin{align*}
        |f(x) - f(c)| &\leq |f(x) - f_N(x)| + |f_N(c) - f(c)| + |f_N(x) - f_N(c)| \\
        &< \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3} \\
        &= \varepsilon
    \end{align*}
\end{proof}

\begin{theorem}
    If $f_n: [a,b] \to \R$ is a sequence of continuous functions such that $f_n$ converges to $f: [a,b] \to \R$ uniformly then
    \begin{equation*}
        \int_a^b f_n(x) \dint x \to \int_a^b f(x) \dint x
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $\varepsilon > 0$. Since $f_n \to f$ uniformly, then $\exists N \in \N$ such that:
    \begin{equation*}
        |f_n(x) - f(x)| < \frac{\varepsilon}{b-a}, \forall n \geq N, \forall x \in [a,b]
    \end{equation*}
    Then,
    \begin{equation*}
        \left |
            \int_a^b f_n(x) \dint x- \int_a^b f(x) \dint x 
        \right | \leq 
        \int_a^b |f_n(x) - f(x)| \dint x 
        <
        \int_a^b \frac{\varepsilon}{b-a} = \varepsilon
    \end{equation*}
\end{proof}

\begin{remark}
    Notationally, this is equivalent to:
    \begin{equation*}
        \lim \limits_{n \to \infty} \int_a^b f_n(x) \dint x = 
        \int_a^b \lim \limits_{n \to \infty} f_n(x) \dint x = 
        \int_a^b f(x) \dint x
    \end{equation*}
\end{remark}

\begin{theorem}
    If $f_n: [a,b] \to \R$ is a sequence of continuous differentiable functions, $f: [a,b] \to \R$, $g:[a,b] \to \R$, and:
    \begin{enumerate}
        \item $f_n$ converges to $f$ pointwise,
        \item $f'_n$ converges to $g$ uniformly,
    \end{enumerate}
    then $f$ is continuously differentiable on $[a,b]$ and $g(x) = f'(x)$.
\end{theorem}

\begin{proof}
    By the Fundamental theorem of Calculus,
    \begin{equation*}
        f_n(x) - f(x) = \int_a^x f'_n(t)\dint t, \forall n \in \N, \forall x \in [a,b]
    \end{equation*}
    Thus, by the previous two theorems,
    \begin{align*}
        f(x) - f(a) &= \lim \limits_{n \to \infty} (f_n(x) - f_n(a)) \\
        &= \lim \limits_{n \to \infty} \int_a^x f'_n(t) \dint t \\
        &= \int_a^x g(t) \dint t
    \end{align*}
    Therefore, $f(x) = f(a) + \int_a^x g(t) \dint t$. Thus, by the Fundamental theorem of calculus, $f$ is differentiable and $f'(x) = (\int_a^x g(t) \dint t)' = g(x)$.
\end{proof}

\section{Power series}

Power series are a natural generalization of polynomials.

\begin{definition}[Power series]
    A power series about $x_0$ is series of the form:
    \begin{equation*}
        \sum \limits_{n = 0}^\infty a_n(x - x_0)^n
    \end{equation*}
\end{definition}

\begin{remark}
    Under this setting, it is common to take $(x-x_0)^0 = 1$. Even though, $0^0$ is indeterminate.
\end{remark}

It is clear from the definition of power series that the series converges for $x=x_0$ to $a_0$. One question follows naturally from this observation: ``for what values of $x$ the power series converges?''

\begin{definition}[Radius of convergence]
    A power series $f(x) = \sum_{n=0}^\infty a_n(x - x_0)^n$ is said to have radius of convergence $\rho$ if $f(x)$ converges absolutely if $|x-x_0| < \rho$ and diverges $|x - x_0| > \rho$, with $\rho \in \R_+\cup \{ +\infty\}$.
\end{definition}

Suppose $\sum a_n(x-x_0)^n$ is a power series with radius of convergence $\rho$. Then, define $f: (x_0 - \rho, x_0 + \rho) \to \R$ such that:
\begin{equation*}
    f(x) := \sum \limits_{n=0}^\infty a_n (x - x_0)^n
\end{equation*}

So, $f(x)$ is a limit of a sequence of functions $f_m(x)$, \emph{i.e.}:

\begin{equation*}
    f(x) = \lim \limits_{m \to \infty} f_m(x)
\end{equation*}

Where,

\begin{equation*}
    f_m(x) = \sum \limits_{n=0}^m a_n(x-x_0)^n
\end{equation*}

\begin{theorem}
    Consider a power series $f(x) = \sum_{n=0}^\infty a_n (x - x_0)^n$, and define \\ $\rho = 1/\limsup_{n \to \infty} |a_n|^{1/n}$, with the convention that $1/0=\infty$ and $1/\infty=0$. Then $\rho$ is the radius of convergence of the $f(x)$. And,
    \begin{itemize}
        \item $f(x)$ converges absolutely for each $x \in (x_0 - \rho, x_0 + \rho)$,
        \item $f(x)$ converges uniformly on any closed interval $[a,b] \subseteq (x_0 - \rho, x_0 + \rho)$,
        \item $f(x)$ diverges for any $x \not \in [x_0 -\rho, x_0 + \rho]$, provided $\rho$ is finite.
    \end{itemize}
\end{theorem}

\begin{proof}
    Define $r := 1/\limsup |a_n|^{1/n}$, in order to apply the Root test on $f(x)$, set:
    \begin{equation*}
       s(x) := \limsup \limits_{n \to \infty} |a_n(x - x_0)^n|^{1/n} = |x-x_0| \cdot \limsup \limits_{n \to \infty} |a_n|^{1/n}
    \end{equation*}
    Then, consider three possibilities:
    \begin{enumerate}
        \item If $r = 0$ then $s(x) = \infty$, so the series does not converge for any $ x \neq x_0$. Hence, the radius of convergence is $\rho = r = 0$,
        \item If $r = \infty$ then $s(x) = 0$, and the series converges - by the Root test - for any $x \in \R$. Hence, the radius of convergence is $\rho = r = \infty$,
        \item If $r \in (0, \infty)$, then $s(x) = |x-x_0|/r$. \\
        So, $s(x) < 1$ if and only if $|x - x_0| < r$, which implies the series converges absolutely if $x \in (x_0 - r, x_0 + r)$ and $\rho = r$. \\ 
        On the other hand, if $|x-x_0| > r$ then $s(x) > 1$, so the series diverges absolutely if $x \not \in [x_0 - r, x_0 + r]$.\\ 
        Finally, let $[a,b] \subseteq (x_0 - \rho, x_0 + \rho)$, choose $x_1 \in (x_0 - \rho, x_0 + \rho)$ such for any $x \in [a,b]$ we have $|x - x_0| \leq |x_1 - x_0|$. Set $M_n = |a_n||x_1 - x_0|^n$. From the first result of this theorem, we have $\sum M_n$ converges. Since $|a_n(x - x_0)^n| \leq M_n, \forall n \in N$ and $x \in [a,b]$. So, from the Weierstrass M-test the series $f(x)$ converges uniformly on $[a,b]$.
    \end{enumerate}
\end{proof}

From the definition of the radius of convergence, if $\rho = 0$ then $f(x)$ converges only for $x = x+0$. Else, if $\rho = \infty$, then $f(x)$ converges for any $x \in \R$.

\begin{theorem}
    If a power series $\sum_{n=0}^\infty a_n x^n$ converges absolutely at a point $c > 0$, then the series converges uniformly on a closed interval $[-c, c]$.
\end{theorem}

\begin{proof}
    This is a straightforward implication of Weierstrass M-test. First, define $f_n: S \to \R$, with $f_n(x) = a_n x^n$. Then, for $x = c, f_n(c) = a_n c^n$, so $|a_n c^n|$ is finite, hence the sequence $(|f_n|)$ is bounded at $x = c$. We may write:
    \begin{equation*}
        |f_n(c)| \leq M_n, \forall n \in \N \cup \{0\}
    \end{equation*}
    Since the series converges absolutely at $x = c$, then - by the comparison test - $\sum_{n=1}^\infty M_n$ also converges. \\
    Furthermore, if $x \in (-c, c)$ then $|a_n x^n| \leq |a_n c^n|$. So, for $S = (-c, c)$
    \begin{enumerate}
        \item 
            \begin{equation*}
                |f_n(x)| = |a_n x^n| < M_n, \forall x \in S
            \end{equation*}
        \item 
            \begin{equation*}
                \sum \limits_{n = 1}^\infty M_n \text{ converges}
            \end{equation*}            
    \end{enumerate}
    So, by the Weierstrass M-test:
    \begin{enumerate}
        \item $\sum_{n=0}^\infty f_n(x)$ converges absolutely $\forall x \in S$
        \item Let $f(x) = \sum_{n=0}^\infty f_n(x) = \sum_{n=0}^\infty a_n x^n$, then the series converges uniformly on $S = (-c, c)$.
    \end{enumerate}
\end{proof}

It is important to notice, that if a series converges conditionally on $x = c$, then it may diverge on $x = -c$, \emph{e.g.:}

\begin{equation*}
    \sum \limits_{n=1}^\infty \frac{(-1)^n x^n}{n}
\end{equation*}

converges on $x = 1$, but not on $x = -1$. More importantly, it converges uniformly on $[0,1]$. 

\begin{lemma}[Abel's lemma]
    Let $(b_n)$ be a non-negative and monotone decreasing sequence. And consider $\sum_{n=1}^\infty a_n$ a series, whose partial sums are bounded, \emph{i.e.}:
    \begin{equation*}
        \left |
            \sum \limits_{n=1}^m a_n
        \right |
        \leq A, \forall m \in N
    \end{equation*}
    Then, 
    \begin{equation*}
        \left |
            \sum \limits_{n=1}^m a_n b_n
        \right |
        \leq A b_1, \forall m \in \N
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $s_m = \sum_{n=1}^m a_n$ and set $s_0 = 0$, then:
    \begin{align*}
        \left |
            \sum \limits_{i=1}^n a_i b_i
        \right |
        &= \left |
            \sum \limits_{i=1}^{n}b_i (s_i - s_{i-1})
        \right | \\
        &= \left |
            \sum \limits_{i=1}^{n}b_i s_i
            - \sum \limits_{j=0}^{n-1}b_{j+1} s_{j}
        \right | \\
        &= \left |
            b_n s_n + \sum \limits_{i=1}^{n-1} s_i(b_i - b_{i+1})
        \right | \\
        &\leq \left |
            A b_n + \sum \limits_{i=1}^{n-1} A (b_i - b_{i+1})
        \right | \\
        &= \left |
            A b_n + A (b_1 - b_n)
        \right | = |A b_1| = A b_1
    \end{align*}
\end{proof}

\begin{theorem}[Abel's theorem]
    Let $f(x) = \sum_{n=0}^\infty a_n x^n$ be a power series which converges on $x = c > 0$. Then, the series converges uniformly on the interval $[0, c]$. \\
    Similarly, if it converges on $x = -c$, then it converges uniformly on $[-c, 0]$, with $c > 0$. 
\end{theorem}

\begin{proof}
    First, notice that:
    \begin{equation*}
        f(x) = \sum \limits_{n=0}^\infty a_n x^n = \sum \limits_{n=0}^\infty (a_n c^n) \left(\frac{x}{c}\right)^n
    \end{equation*}
    Let $\varepsilon > 0$, then by the Cauchy criterion, the series converges if there exists $N$ such that:
    \begin{equation*}
        \left |
            (a_{m+1}c^{m+1})\left(\frac{x}{c}\right)^{m+1}
            + ...
            + (a_{n}c^{n})\left(\frac{x}{c}\right)^{n} 
        \right |
        < \varepsilon
    \end{equation*}
    with $n > m \geq N$. Because the series converges at $x = c$, then there exists $N$ such that:
    \begin{equation*}
        \left |
            a_{m+1}c^{m+1} + ... + a_n c^n
        \right | <  \varepsilon
    \end{equation*}
    if $n > m \geq N$. First, take $\varepsilon/2$ as a bound for $\sum_{i=1}^\infty a_{m+i}c^{m+i}$. Second, notice $(x/c)^{m+j}$ is monotone decreasing on $x \in [0, c]$. Hence, 
    \begin{align*}
        \left |
            (a_{m+1}c^{m+1})\left(\frac{x}{c}\right)^{m+1}
            + ...
            + (a_{n}c^{n})\left(\frac{x}{c}\right)^{n} 
        \right |
        &\leq \frac{\varepsilon}{2}\left(\frac{x}{c}\right)^{m+1}\\
        &< \varepsilon
    \end{align*}
\end{proof}

\begin{remark}
    If a power series $f(x) = \sum_{n=0}^\infty a_n (x - x_0)^n$ converges at some point $x_1 > x_0$, then $f(x)$ converges uniformly on $[x_0, x_1]$ and absolutely on $[x_0, x_1)$. \emph{Id est}, the series may not converge absolutely on $x = x_1$.
\end{remark}

Before evaluating the differentiability of power series, we must first show that the radius of convergence of a power series remains unchanged by term-by-term differentiation.

\begin{lemma}
    \label{lemma:limsup_power_series_derivative}
    If $a_n \in \R, \forall n \in \N$, then:
    \begin{equation*}
        \limsup\limits_{n\to \infty} (n|a_n|)^{1/n} = \limsup \limits_{n \to \infty}|a_n|^{1/n}
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $\varepsilon > 0$. Notice $n^{1/n} \to 1$ as $n \to \infty$, choose $N \in \N$ such that $1 - \varepsilon < n^{1/n} < 1 + \varepsilon, \forall n \geq N$, which implies:
    \begin{equation*}
        (1 - \varepsilon)|a_n|^{1/n} (n|a_n|)^{1/n} < (1 + \varepsilon)|a_n|^{1/n}
    \end{equation*}
    So, $\sup_{k > n}\{ k |a_k|^{1/k}\} \leq (1 + \varepsilon)\sup_{k > n} \{|a_k|^{1/k}\}$. Taking the limit as $n \to \infty$ leads to:
    \begin{equation*}
        x: \limsup \limits_{n \to \infty} (n |a_n|^{1/n}) \leq (1+\varepsilon)\limsup \limits_{n \to \infty} |a_n|^{1/n} =: (1+\varepsilon)y
    \end{equation*}
    Taking the limit as $n \to \infty$ implies $x \leq y$. Following a similar strategy for $(1-\varepsilon)$ instead of $(1+\varepsilon)$ leads to $x \geq y$. Hence, $x = y$
\end{proof}

\begin{theorem}[Derivative of a power series]
    Consider a power series $f(x) = \sum_{n=0}^\infty a_n(x - x_0)^n$ with radius of convergence $\rho >0$. Then, $f'(x) = \sum_{n=1}^\infty n a_n (x - x_0)^{n-1}, \forall x \in (x_0 - \rho, x_0 + \rho)$.
\end{theorem}

\begin{proof}
    Define $S = (x_0 - \rho, x_0 + \rho)$ and suppose $[a,b] \subseteq S$. By Lemma \ref{lemma:limsup_power_series_derivative}, the radius of convergence of the power series $g(x) = \sum_{n=1}^\infty n a_n (x - x_0)^n$ is $\rho$. Thus, $g(x)$ converges absolutely on $S$ and uniformly on $[a,b]$.\\
    Consider the derived series $f^{*}(x) = \sum_{n=1}^\infty n a_n (x - x_0)^{n-1}$. Then, $f^{*}(x_0) = a_1$. If $x \in S \setminus \{x_0\}$, then $f^{*}(x) = g(x)/(x - x_0)$ and, again, $f^{*}(x)$ converges absolutely. So, the radius of convergence of $f^{*}(x)$ is at least $\rho$. Hence, $f(x)$ is differentiable on $[a,b]$ and $f'(x) = f^{*}(x), \forall x \in [a,b]$. Since $[a,b] \subseteq S$ then $f'(x) = f^{*}(x), \forall x \in S$.
\end{proof}

\begin{corollary}
    \label{corollary:k-th_der_power_series}
    If $f(x) = \sum_{n=0}^\infty a_n (x-x_0)^n$ has a positive radius of convergence, $\rho$, then $f(x) \in \mathcal{C}^\infty(x_0 - \rho, x_0 + \rho)$ and
    \begin{equation*}
        f^{(k)}(x) = \sum \limits_{n = k}^\infty \frac{n!}{(n-k)!}a_n (x - x_0)^{n-k}, \forall x \in (x_0 - \rho, x_0 + \rho) \text{ and } k \in \N
    \end{equation*}
\end{corollary}

\begin{proof}
    \textbf{Proof by induction:}\\
    From the previous theorem, and knowing $0! = 1$, then:
    \begin{equation*}
        f^{(1)}(x) = n a_n (x - x_0)^{n-1}
    \end{equation*}
    So, the relation is valid for $k = 1$. If the relation is true for some $k \in \N$ and $x \in (x_0 - \rho, x_0 + \rho)$ then $f^{(k)}(x)$ is a power series with radius of convergence $\rho$, hence:
    \begin{align*}
        f^{(k+1)}(x) = (f^{(k)}(x))' &= \left(
            \sum \limits_{n=k}^\infty \frac{n!}{(n-k)!}a_n (x-x_0)^{n-k}
        \right)' \\
        &= \sum \limits_{n = k + 1}^\infty \frac{n!}{(n-k-1)!}a_n (x - x_0)^{n-k-1}, \forall x \in (x_0 - \rho, x_0 + \rho)
    \end{align*}
    Hence, by induction the relation is valid for all $k \in \N$.
\end{proof}

\begin{theorem}
    Consider a power series $f(x) = \sum_{n=0}^\infty a_n(x - x_0)^n$ with radius of convergence $\rho \in (0, \infty)$, and $a,b \in \R$ such that $x_0 - \rho < a < b < x_0 + \rho$, then:
    \begin{equation*}
        \int_a^b f(x) \dint x = \sum \limits_{n = 0}^\infty a_n \int_a^b (x - x_0)^n \dint x
    \end{equation*}
\end{theorem}

\begin{proof}
    By Abel's theorem, $f(x)$ converges uniformly on $[a,b]$ since $[a,b] \subseteq (x_0 -\rho, x_0 + \rho)$. Hence, by term-by-term integration:
    \begin{equation*}
        \int_a^b f(x) \dint x = \sum \limits_{n = 0}^\infty a_n \int_a^b (x - x_0)^n \dint x
    \end{equation*}
\end{proof}

\begin{theorem}[Weierstrass approximation theorem]
    If $f \in C([a,b])$, there exists a sequence of polynomials $\{ P_n\}$ such that $P_n \to f$ uniformly on $[a,b]$.
\end{theorem}

\begin{theorem}
    Define $c_n := (\int_{-1}^1 (1 - x^2)^n \dint x)^{-1} > 0$, and let $Q_n(x) = c_n(1 - x^2)^n$. Then:
    \begin{enumerate}
        \item $\int_{-1}^1 Q_n(x) \dint x = 1, \forall n \in \N$,
        \item $Q_n(x) \geq 0, \forall n \in \N, \forall x \in [-1, 1]$, and
        \item $\forall \delta \in (0,1), Q_n \to 0$ uniformly on $\delta \leq |x| \leq 1$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Proving each statement:
    \begin{enumerate}
        \item $\int_{-1}^1 Q_n(x) \dint x = c_n \int_{-1}^1 (1 - x^2)^n \dint x = 1$ by definition of $c_n$,
        \item Since $c_n$ is strictly positive, notice $(1 - x^2) \geq 0, \forall x \in [-1,1]$. Any non-negative number to the $n$-th power will result in a non-negative number (since $n \in \N $). So $Q_n(x) \geq 0, \forall n \in \N, \forall x \in [-1, 1]$,
        \item First, notice:
            \begin{equation*}
                (1 - x^2)^n \geq 1 - nx^2, \forall n \in \N, \forall x \in [-1, 1]
            \end{equation*}
            This can be proven by induction, beyond the scope of this particular proof. And $g(x) = (1 - x^2)^n - (1 - n x^2)$ satisfies $g(0) = 0$. Additionally, $g'(x) = n 2 x (1 - (1 - x^2)^{n-1}) \geq 0$ in $[0,1]$. Thus, $g(x) \geq 0$ by the Mean value theorem. So,
            \begin{align*}
                \frac{1}{c+n} &= \int_{-1}^1 (1 - x^2)^n \dint x \\
                &= 2 \int_0^1 (1 - x^2)^n \dint x \\
                &> 2 \int_0^{1/\sqrt{n}}(1 - x^2)^n \dint x \\
                &\geq 2 \int_0^{1/\sqrt{n}}(1 - n x^2) \dint x \\
                &= 2 \left(
                    \frac{1}{\sqrt{n}} - \frac{n}{3}n^{-3/2}
                \right) \\
                &= \frac{4}{3}\sqrt{n}
                &>\sqrt{n}
            \end{align*}
            Therefore $c_n < \sqrt{n}$. Let $\delta > 0$. First, notice $\lim_{n \to \infty} \sqrt{n}(1 - \delta^2)^n = 0$. By the root test,
            \begin{align*}
                \lim \limits_{n \to \infty} (\sqrt{n} (1 - \delta^2)^n)^{1/n} &= \lim \limits_{n \to \infty} (n^{1/n})^{1/2}(1 - \delta^2) \\
                &= 1 - \delta^2 \\
                &< 1
            \end{align*}
            So, $\lim_{n \to \infty} \sqrt{n}(1 - \delta^2)^n = 0$. Let $\varepsilon > 0$ and choose $N \in \N$ such that $\sqrt{n}(1 - \delta^2)^n < \varepsilon, \forall n \geq N$. Then, $\forall n \geq N, \forall \delta \leq |x| \leq 1, |c_n(1-x^2)^n| < sqrt{n}(1 - x^2)^n \leq \sqrt{n}(1 - \delta^2)^n < \varepsilon$.
    \end{enumerate}
\end{proof}
\vspace{1em}
Finally, we return to proving Weierstrass approximation theorem.
\begin{proof}
    Suppose $f \in C([0, 1])$, with $f(0) = f(1) = 0$. $f$ can be extended to $C(\R)$ by setting $f(x) = 0, \forall x \not \in [0,1]$. Define:
    \begin{align*}
        P_n(x) &= \int_0^1 f(t) Q_n(t - x) \dint t \\
        &= \int_0^1 f(t) c_n (1 - (t - x)^2)^n \dint t
    \end{align*}
    Note that $P_n(x)$ is in fact a polynomial. Additionally, note that for $x \in [0,1]$,
    \begin{align*}
        P_n(x) &= \int_0^1 f(t) Q_n(t - x) \dint t \\
        &= \int_{-x}^{1 - x} f(x + t)Q_n(t) \dint t \\
        &= \int_{-1}^1 f(x + t)Q_n(t)\dint t
    \end{align*}
    The second equality follows from change of variables, the third one comes from $f(x + t) = 0, \forall t \not \in [-x, 1 - x]$.
    Now, it is necessary to show $P_n \to f$ uniformly on $[0,1]$. Let $\varepsilon > 0$, since $f$ is uniformly continuous on $[0,1], \exists \delta > 0$ such that $\forall |x-y| \leq \delta, |f(x) - f(y)| < \varepsilon/2$. Let $C = \sup \{f(x):x \in [0,1]\}$, which exists by the Min/Max theorem. Choose $N \in \N$ such that $\sqrt{n}(1 - \delta^2)^n < \varepsilon/(8C)$. Thus, $\forall n \geq N, \forall x \in [0,1]$,
    \begin{align*}
        |P_n(x) - f(x)| &= \left |
            \int_{-1}^1 (f(x-t) - f(t))Q_n(t) \dint t
        \right | \\
        & \leq \int_{-1}^1 |f(x - t) - f(x)| Q_n(t) \dint t \\
        & \leq \int_{|t| \leq \delta} |f(x - t) - f(x)| Q_n(t) \dint t + \int_{\delta \leq |t| \leq 1} |f(x - t) - f(x)| Q_n(t) \dint t \\
        & \leq \frac{\varepsilon}{2}\int_{|t| \leq \delta} Q_n(t) \dint t + \sqrt{n}(1 - \delta^2)^n \int_{\delta \leq |t| \leq 1} 2C \dint t \\
        &< \frac{\varepsilon}{2} + 4C \sqrt{n}(1 - \delta^2)^n \\
        &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
        &= \varepsilon
    \end{align*}
\end{proof}

\section{Analytic functions}

Once power series functions have been introduced, it is possible to study functions that can be represented as power functions. First, let us define:

\begin{definition}[Analytic function]
    A function, $f$, is said to be \emph{analytic} on a non-empty, open interval $(a,b)$ if, and only if, given $x_0 \in (a,b)$ there exists a power series centered on $x_0$ which converges to $f$ near $x_0$. \emph{Id est}, there exists $(a_n)_{n=0}^\infty$, $c, d \in (a,b)$ and $c < x_0 < d$ such that:
    \begin{equation*}
        f(x) = \sum \limits_{n=0}^\infty a_n (x - x_0)^n, \forall x \in (c,d)
    \end{equation*}
\end{definition}

\begin{remark}
    An analytic function is said to belong to the analytic class, $\mathcal{C}^\omega(S)$.
\end{remark}

\begin{theorem}[Uniqueness]
    Let $c, d \in \R \cup \{-\infty, +\infty\}$ with $d > c$. Consider $x_0 \in (c, d)$ and suppose $f: (c,d) \to \R$. If $f(x) = \sum_{n=0}^\infty a_n (x - x_0)^n, \forall x \in (c, d)$ then $f \in \mathcal{C}^\omega(c,d)$ and:
    \begin{equation*}
        a_n = \frac{f^{(n)}(x_0)}{n!}, \text{ with } k \in \{0, 1, ...\}
    \end{equation*}
\end{theorem}

\begin{proof}
    From the definition of the power series, $f(x_0) = a_0$. Additionally, $(c,d) \subseteq (x_0 - \rho, x_0 + \rho)$ where $\rho > 0$ is the radius of convergence of the power series. From Corollary \ref{corollary:k-th_der_power_series}, $f \in \mathcal{C}^\infty(c,d)$ and:
    \begin{equation*}
        f^{(k)}(x) = \sum \limits_{n=k}^\infty \frac{n!}{(n-k)!}a_n (x - x_0)^{n-k}, \forall x \in (c,d)
    \end{equation*}
    Now, if $x = x_0$:
    \begin{itemize}
        \item if $n > k$ then the right-hand side of the previous equation equals zero;
        \item else, if $n = k$ then $f^{(k)}(x_0) = k! a_k, \forall k \in \N$
    \end{itemize}
\end{proof}

\begin{theorem}[Taylor's theorem]
    Suppose $f: [a,b] \to \R$ and $f \in \mathcal{C}^{n+1}(a,b)$. Given $x, x_0 \in [a,b]$, there exists $c \in (a,b)$ such that:
    \begin{equation}
        f(x) = \sum \limits_{k = 0}^n \frac{1}{k!}f^{(k)}(x_0)(x-x_0)^k + \frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}
    \end{equation}
    Denote the large sum as $P_n(x)$ and the remainder as $R_n(x)$.
\end{theorem}

\begin{proof}
    Let $x, x_0 \in [a,b]$. If $x = x_0$ then any $c$ satisfies the theorem. Suppose $x \neq x_0$. Define:
    \begin{equation*}
        M_{x, x_0} = \frac{f(x) - P_n(x)}{(x-x_0)^{n+1}}
    \end{equation*}
    Then, $f(x) = P_n(x) + M_{x, x_0}(x-x_0)^{n+1}$. And $f^{(k)}(x_0) = P^{(k)}(x_0), \forall 0 \leq k \leq n$. Let $g(s) = f(x) - P_n(s) - M_{x, x_0}(s-x_0)^{n+1}$, then:
    \begin{eqnarray*}
        g(x_0) &=& f(x_0) - P_n(x_0) - M_{x,x_0}(x-x_0)^{n+1} = 0 \\
        g'(x_0) &=& f'(x_0) - P'_n(x_0) - M_{x,x_0}(n+1)(x-x_0)^{n} = 0 \\
        g''(x_0) &=& f''(x_0) - P''_n(x_0) - M_{x,x_0}(n+1)n(x-x_0)^{n-1} = 0 \\
        &...& \\
        g^{(n)}(x_0) &=& f^{(n)}(x_0) - P^{(n)}_n(x_0) - M_{x,x_0}(n+1)!(x-x_0) = 0
    \end{eqnarray*}
    Since $g(x) = 0$ and $g(x_0) = 0$, by the Mean value theorem, there exists $x_1 \in (x_0, x)$ such that $g'(x_1) = 0$. Thus, $g'(x_0) = 0$ and $g'(x_1) = 0$. By consequence, there exists $x_2 \in (x_0, x_1)$ such that $g''(x_2) = 0$. Preceding similarly, we find $x_n \in (x_0, x_{n-1})$ such that $g^{(n)}(x_n) = 0$. Finally, $g^{(n)}(x_0) = 0$ and $g^{(n)}(x_n) = 0$ which implies $\exists c \in (x_0, x_n)$ such that $g^{(n+1)}(c) = 0$. So,
    \begin{equation*}
        \frac{\dint ^{n+1}}{\dint s^{n+1}} M_{x, x_0}(s - x_0)^{n+1} = M_{x, x_0}(n+1)!
    \end{equation*}
    Additionally, $P_n^{(n+1)}(c) = 0$ since $P_n(x)$ is a polynomial with degree $n$. Hence,
    \begin{equation*}
        0 = g^{(n+1)}(c) = f^{(n+1)}(c) - M_{x, x_0}(n+1)! \Longrightarrow M_{x, x_0} = \frac{f^{(n+1)}(c)}{(n+1)!}
    \end{equation*}
    Thus,
    \begin{equation*}
        f(x) = P_n(x) + \frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}
    \end{equation*}
\end{proof}

\begin{definition}[Taylor expansion]
    Consider $f \in \mathcal{C}^\infty(a,b)$ and $x_0 \in (a,b)$. The \emph{Taylor expansion} (or Taylor Series) of $f$ around $x_0$ is:
    \begin{equation*}
        \sum \limits_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n
    \end{equation*}
    No convergence is implied or assumed.
\end{definition}

\begin{remark}
    The Taylor expansion around $x_0 = 0$ is called \emph{Maclaurin} series of $f$.
\end{remark}