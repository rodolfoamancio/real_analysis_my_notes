\chapter{Metric spaces and topology}

Metric spaces allow to evaluate the previous results of analysis for functions of one variable, also referred as analysis on the line, to multiple dimensions. First, it is necessary to define what is a metric:

\begin{definition}[Metric]
    Given a set $M$, a function $d: M \times M \to \R$ is a \emph{metric} on $M$ if $\forall x, y \in M$ the following conditions are valid:
    \begin{enumerate}
        \item Positive definiteness: $d(x,y) \geq 0$ and $d(x,y) = 0$ if and only if $x=y$,
        \item Symmetry: $d(x,y) = d(y,x)$,
        \item Triangle inequality: $d(x,y) \leq d(x,z) + d(z,y), \forall z \in M$.
    \end{enumerate}
\end{definition}

A metric space is defined as a tuple formed by a set $M$ and a metric $d$: $(M, d)$.

Consider the following examples:

\begin{eg}
    For $M = \R^2$, is $d(x, y) = \sqrt{(x_1 -  x_2)^2 + (y_1 - y_2)^2}$ a valid metric? \\
    \textbf{Solution} \\
    Evaluating the three properties a \emph{metric} must satisfy:
    \begin{enumerate}
        \item First, $(x_1 - x_2)^2$ and $(y_1 - y_2)$ are strictly non-negative for any values of $x,y \in \R^2$. Since $\sqrt{z} \geq 0, \forall z \in R : z \geq 0$ then $d(x, y) \geq 0$. Second, $d(x, y) = 0$ which implies $\sqrt{(x_1 - x_2)^2 + (y_1- y_2)^2} = 0$, so $(x_1 - x_2)^2 = - (y_1 - y_2)^2$. Since $z^2 \geq 0, \forall z \in \R$ then the only possible solution for this equation is $x_1 = x_2$ and $y_1 = y_2$ and $d(x, y) = 0 \Longleftrightarrow x = y$,
        \item Since $(x_1 - x_2)^2 = (x_2 - x_1)^2, \forall x_1, x_2 \in \R$ and the same is valid for the second argument inside the square root, then $d(x, y) = d(y, x)$,
        \item For the third point, start by considering $x = (x_1, y_1), y = (x_2, y_2), z = (x_3, y_3)$. Using a little of linear algebra, $x \cdot y = x_1x_2 + y_1y_2$ is the dot product and $|x| = \sqrt{x \cdot x}$. Define $u = x - y$ and $v = y - z$, so $u + v = x - z$. With, $d(x,y) = |u|$ and $d(y,z) = |v|$. Hence, the triangle inequality may be written as:
        \begin{equation*}
            |u + v| \leq |u| + |v|, \forall u, v \in \R
        \end{equation*}
        Squaring both sides leads to:
        \begin{align*}
            |u + v|^2 &\leq |u|^2 + 2|u||v| + |v|^2 \\
            (u + v) \cdot (u + v) &\leq |u|^2 + 2|u||v| + |v|^2 \\
            |u|^2 + 2u\cdot v + |v|^2 &\leq |u|^2 + 2|u||v| + |v|^2 \\
            u\cdot v &\leq |u||v|
        \end{align*}
        From linear algebra, $u \cdot v = |u||v|\cos(\theta)$ where $\theta$ is the angle betweeen the two vectors, as $-1 \leq \cos(\theta) \leq 1 ,\forall \theta \in [-\pi, \pi]$ then the relation above is true and the condition is verified.
    \end{enumerate}
\end{eg}

\begin{eg}
    Consider $f, g \in \mathcal{C}([0,1])$. Is $d(f,g) = \sup\{|f(x) - g(x)|: x \in [0,1]\}$ a metric?
    \textbf{Solution}
    Consider each property:
    \begin{enumerate}
        \item Since $|x| \geq 0, \forall x \ in \R$ so $|f(x) - g(x)| \geq 0, \forall x \in [0,1]$ and consequently its supremum, hence the first condition is satisfied;
        \item Notice that $|f(x) - g(x)| = |-1*(g(x) - f(x))|$ since $|x||y| = |xy| \forall x,y \in \R^2$. Thus, $|f(x) - g(x)| = |-1*(g(x) - f(x))| = |-1||g(x) - f(x)| = |g(x) - f(x)|$. Since $|x| = |-x|, \forall x \in \R$, then $|f(x) - g(x)| = |g(x) - f(x)|$;
        \item Finally, let $h \in \mathcal{C}([0, 1])$ with $\alpha = \sup \{|f(x) - h(x)|: x \in [0,1] \}$ and $\beta = \sup \{|h(x) - g(x)|: x \in [0,1] \}$. From the triangle inequality: $\alpha + \beta \geq |f(x) - h(x)| + |h(x) - g(x)| = |f(x) - g(x)|, \forall x \in [0,1]$ and $\alpha + \beta$ is an upper bound for $|f(x) - g(x)|$. Thus, $\alpha + \beta \geq \sup\{|h(x) - g(x)|: x \in [0,1]\}$.
    \end{enumerate}
\end{eg}

\begin{definition}[Convergence]
    A sequence $(x_n) $ in a metric space $(X, d)$ converges to $x \in X$ if for all $\varepsilon > 0$ there exists $N \in \N$ such that $d(x_m, x) < \varepsilon$ if $n \geq N$.
\end{definition}

\begin{definition}[Cauchy sequence]
    A sequence $(x_n)$ in a metric space $(X, d)$ is Cauchy if for all $\varepsilon > 0$ there exists $N \in \N$ such that $d(x_n, x_m) < \varepsilon$ provided $m, n \geq N$.
\end{definition}

\begin{definition}[Completeness in metric space]
    A metric space $(X, d)$ is complete if every Cauchy sequence $(x_n) \subseteq X$ converges to any element $x \in X$
\end{definition}

\begin{eg}
    Consider a sequence $(x_n)$ in a metric space $(X, d)$ which converges to $x \in X$. Show that $(x_n)$ is also Cuachy.\\
    \textbf{Solution}
    Let $\varepsilon > 0$, then $(x_n)$ is Cauchy if there exists $N \in \N$ such that $d(x_n, x_m) < \varepsilon, \forall n, m \geq N$. Since the sequence is convergent, there exists $L \in \N$ such that $d(x_{l}, x) < \varepsilon', \forall l \geq L$ given an arbitrary $\varepsilon' > 0$. From the triangle inequality, $d(x_n, x_m) \leq d(x_n, x) + d(x_m, x)$. Choose $\varepsilon' = \varepsilon/2$, then $d(x_n, x_m) \leq d(x_n, x) + d(x_m, x) < \varepsilon/2 + \varepsilon/2 = \varepsilon$, for $m, n \geq L$. Hence, $d(x_n, x_m) < \varepsilon$ and the sequence is Cauchy.
\end{eg}